{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Декомпозиция Клементса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clements unitary decomposition\n",
    "\"\"\"\n",
    "\n",
    "from typing import NamedTuple, List, Tuple\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def mzi_matrix(\n",
    "    dim: int,\n",
    "    target: Tuple[int, int],\n",
    "    theta: float,\n",
    "    phi: float,\n",
    "    dtype=torch.complex128,\n",
    "    device='cpu'\n",
    ") -> Tensor:\n",
    "    \"\"\"Matrix form of a Mach-Zehnder interferometer using PyTorch.\"\"\"\n",
    "    m, n = target\n",
    "    if n - m != 1:\n",
    "        raise ValueError(\"m and n must be consecutive integers.\")\n",
    "\n",
    "    u_mzi = torch.eye(dim, dtype=dtype, device=device)\n",
    "\n",
    "    theta_t = torch.tensor(theta, dtype=torch.float32, device=device, requires_grad=False)\n",
    "    phi_t = torch.tensor(phi, dtype=torch.float32, device=device, requires_grad=False)\n",
    "\n",
    "    global_phase = torch.exp(1j * theta_t)\n",
    "    cos_t = torch.cos(theta_t)\n",
    "    sin_t = torch.sin(theta_t)\n",
    "    exp_phi = torch.exp(1j * phi_t)\n",
    "\n",
    "    u_mzi[m, m] = global_phase * exp_phi * cos_t\n",
    "    u_mzi[m, n] = global_phase * 1j * sin_t\n",
    "    u_mzi[n, m] = global_phase * exp_phi * 1j * sin_t\n",
    "    u_mzi[n, n] = global_phase * cos_t\n",
    "\n",
    "    return u_mzi\n",
    "\n",
    "\n",
    "class MachZehnder(NamedTuple):\n",
    "    theta: float\n",
    "    phi: float\n",
    "    target: Tuple[int, int]\n",
    "\n",
    "\n",
    "class Decomposition(NamedTuple):\n",
    "    D: Tensor\n",
    "    circuit: List[MachZehnder]\n",
    "\n",
    "\n",
    "def _is_unitary(U: Tensor, atol=1e-6) -> bool:\n",
    "    I = torch.eye(U.shape[0], dtype=U.dtype, device=U.device)\n",
    "    return torch.allclose(U @ U.conj().T, I, atol=atol)\n",
    "\n",
    "def clements_decomposition(U: Tensor) -> Decomposition:\n",
    "    if not isinstance(U, torch.Tensor):\n",
    "        raise TypeError(\"U must be a torch.Tensor.\")\n",
    "    if not _is_unitary(U):\n",
    "        raise ValueError(\"U must be a unitary matrix.\")\n",
    "\n",
    "    dim = U.shape[0]\n",
    "    dtype = U.dtype\n",
    "    device = U.device\n",
    "\n",
    "    right_sequence: List[MachZehnder] = []\n",
    "    left_sequence: List[MachZehnder] = []\n",
    "    U = U.clone()\n",
    "\n",
    "    for k in range(1, dim):\n",
    "        if k % 2 == 1:\n",
    "            for i, j in zip(range(dim - 1, dim - k - 1, -1), range(k - 1, -1, -1)):\n",
    "                if torch.abs(U[i, j]) < 1e-12:\n",
    "                    continue\n",
    "                if torch.abs(U[i, j + 1]) < 1e-12:\n",
    "                    phi = 0.0\n",
    "                else:\n",
    "                    phi = (torch.angle(U[i, j] / U[i, j + 1]) - torch.pi / 2) % (2 * torch.pi)\n",
    "                theta = torch.arctan2(torch.abs(U[i, j]), torch.abs(U[i, j + 1]))\n",
    "                right_sequence.append(MachZehnder(theta, phi, (j, j + 1)))\n",
    "                mzi = mzi_matrix(dim, (j, j + 1), theta, phi, dtype=dtype, device=device)\n",
    "                U = U @ mzi.conj().T\n",
    "        else:\n",
    "            for i, j in zip(range(dim - k, dim), range(0, k)):\n",
    "                if torch.abs(U[i, j]) < 1e-12:\n",
    "                    continue\n",
    "                if torch.abs(U[i - 1, j]) < 1e-12:\n",
    "                    phi = 0.0\n",
    "                else:\n",
    "                    phi = (torch.angle(U[i, j] / U[i - 1, j]) + torch.pi / 2) % (2 * torch.pi)\n",
    "                theta = torch.arctan2(torch.abs(U[i, j]), torch.abs(U[i - 1, j]))\n",
    "                left_sequence.insert(0, MachZehnder(theta, phi, (i - 1, i)))\n",
    "                mzi = mzi_matrix(dim, (i - 1, i), theta, phi, dtype=dtype, device=device)\n",
    "                U = mzi @ U\n",
    "\n",
    "    D = torch.diag(U).clone()\n",
    "    new_left_sequence: List[MachZehnder] = []\n",
    "\n",
    "    for theta, phi, (m, n) in left_sequence:\n",
    "        new_phi = (torch.pi + torch.angle(D[m]) - torch.angle(D[n])) % (2 * torch.pi)\n",
    "        new_left_sequence.insert(0, MachZehnder(theta, new_phi, (m, n)))\n",
    "        new_beta = torch.angle(D[n]) - 2 * theta\n",
    "        new_alpha = torch.angle(D[n]) - torch.pi - phi - 2 * theta\n",
    "        D[m] = torch.exp(1j * new_alpha)\n",
    "        D[n] = torch.exp(1j * new_beta)\n",
    "\n",
    "    return Decomposition(D=D, circuit=new_left_sequence + right_sequence[::-1])\n",
    "\n",
    "def circuit_reconstruction(decomposition: Decomposition) -> Tensor:\n",
    "    if not isinstance(decomposition, Decomposition):\n",
    "        raise TypeError(\"decomposition must be a Decomposition object.\")\n",
    "\n",
    "    D = decomposition.D\n",
    "    dim = D.shape[0]\n",
    "    dtype = D.dtype\n",
    "    device = D.device\n",
    "\n",
    "    reconstructed_matrix = torch.diag(D)\n",
    "\n",
    "    for theta, phi, (m, n) in decomposition.circuit:\n",
    "        mzi = mzi_matrix(dim, (m, n), theta, phi, dtype=dtype, device=device)\n",
    "        reconstructed_matrix = reconstructed_matrix @ mzi\n",
    "\n",
    "    return reconstructed_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разложение Река"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_matrix(\n",
    "    dim: int,\n",
    "    target: Tuple[int, int],\n",
    "    theta: Tensor,   # теперь принимаем Tensor (для градиентов)\n",
    "    phi: Tensor,     # тоже Tensor\n",
    "    dtype=torch.complex128,\n",
    "    device='cpu'\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Matrix form of a 2-mode unitary transformation T_{m,n}(theta, phi)\n",
    "    used in Reck decomposition.\n",
    "\n",
    "    The 2x2 block is:\n",
    "        [[cos(theta),        -exp(-i*phi) * sin(theta)],\n",
    "         [exp(i*phi) * sin(theta),  cos(theta)]]\n",
    "\n",
    "    Parameters:\n",
    "        dim: размер полной матрицы\n",
    "        target: кортеж (m, n) — индексы мод\n",
    "        theta: угол вращения (скалярный тензор, dtype=float)\n",
    "        phi: фазовый угол (скалярный тензор, dtype=float)\n",
    "        dtype: тип комплексных чисел (например, torch.complex128)\n",
    "        device: устройство\n",
    "\n",
    "    Returns:\n",
    "        Полная dim x dim унитарная матрица.\n",
    "    \"\"\"\n",
    "    m, n = target\n",
    "\n",
    "    # Создаём единичную матрицу нужного размера\n",
    "    T = torch.eye(dim, dtype=dtype, device=device)\n",
    "\n",
    "    # Вычисляем элементы 2x2 блока с использованием torch (дифференцируемо!)\n",
    "    cos_t = torch.cos(theta)\n",
    "    sin_t = torch.sin(theta)\n",
    "    exp_iphi = torch.exp(1j * phi)\n",
    "\n",
    "    # Заполняем 2x2 подматрицу\n",
    "    T[m, m] = cos_t\n",
    "    T[m, n] = -torch.conj(exp_iphi) * sin_t  # = -exp(-i*phi) * sin(theta)\n",
    "    T[n, m] = exp_iphi * sin_t               # =  exp(i*phi) * sin(theta)\n",
    "    T[n, n] = cos_t\n",
    "\n",
    "    return T\n",
    "\n",
    "def reck_decomposition(U: Tensor):\n",
    "    \"\"\"\n",
    "    Reck decomposition of a unitary matrix U into a sequence of\n",
    "    2-mode transformations T_{m,n}(theta, phi) and final phases.\n",
    "    \n",
    "    Returns:\n",
    "        thetas: Tensor of rotation angles (real, shape [K])\n",
    "        phis:   Tensor of phase angles (real, shape [K])\n",
    "        targets: Tensor of mode pairs (int64, shape [K, 2])\n",
    "        phases: Diagonal phase shifts (real, shape [N])\n",
    "    \"\"\"\n",
    "    N = U.shape[0]\n",
    "    device = U.device\n",
    "    dtype_real = torch.float64\n",
    "    dtype_int = torch.int64\n",
    "\n",
    "    # Инициализация списков (лучше, чем cat в цикле)\n",
    "    thetas_list = []\n",
    "    phis_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    U_work = U.clone()\n",
    "\n",
    "    for n in range(N - 1, 0, -1):          # столбцы: справа налево\n",
    "        for m in range(n - 1, -1, -1):     # строки: снизу вверх в столбце n\n",
    "            a = U_work[n, n]\n",
    "            b = U_work[m, n]\n",
    "\n",
    "            r = torch.sqrt(torch.abs(a)**2 + torch.abs(b)**2 + 1e-15)\n",
    "\n",
    "            if r < 1e-12:\n",
    "                theta = torch.tensor(0.0, dtype=dtype_real, device=device)\n",
    "                phi = torch.tensor(0.0, dtype=dtype_real, device=device)\n",
    "            else:\n",
    "                cos_theta = torch.abs(a) / r\n",
    "                cos_theta = torch.clamp(cos_theta, -1.0, 1.0)\n",
    "                theta = torch.acos(cos_theta)\n",
    "                phi = torch.angle(a) - torch.angle(b)  # ВАЖНО: правильный знак!\n",
    "\n",
    "            thetas_list.append(theta)\n",
    "            phis_list.append(phi)\n",
    "            targets_list.append(torch.tensor([m, n], dtype=dtype_int, device=device))\n",
    "\n",
    "            # Строим T_{m,n}\n",
    "            T_mn = t_matrix(N, target=(m,n), theta=theta, phi=phi)\n",
    "\n",
    "            U_work = T_mn @ U_work\n",
    "\n",
    "    # Собираем тензоры\n",
    "    thetas = torch.stack(thetas_list) if thetas_list else torch.empty(0, dtype=dtype_real, device=device)\n",
    "    phis = torch.stack(phis_list) if phis_list else torch.empty(0, dtype=dtype_real, device=device)\n",
    "    targets = torch.stack(targets_list) if targets_list else torch.empty((0, 2), dtype=dtype_int, device=device)\n",
    "\n",
    "    phases = torch.angle(torch.diag(U_work))  # вещественные фазы\n",
    "\n",
    "    return thetas, phis, targets, phases\n",
    "\n",
    "def get_matrix(\n",
    "    dim: int, \n",
    "    phases: Tensor,\n",
    "    phis: Tensor,\n",
    "    thetas: Tensor,\n",
    "    targets: Tensor,\n",
    "    dtype=torch.complex128,\n",
    "    device='cpu'\n",
    ") -> Tensor:\n",
    "    U = torch.diag(torch.exp(1j * phases.to(dtype))).to(device)\n",
    "    \n",
    "    # Применяем в ОБРАТНОМ порядке! (Reck применял T @ U, значит U = T1† T2† ... D)\n",
    "    for i in range(len(thetas) - 1, -1, -1):\n",
    "        theta = thetas[i]\n",
    "        phi = phis[i]\n",
    "        m, n = targets[i].tolist()\n",
    "        T_mn = t_matrix(dim, target=(m,n), theta=theta, phi=phi)\n",
    "\n",
    "        U = T_mn.adjoint() @ U\n",
    "\n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проверка (Клементс)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Unitary:\n",
      " [[-0.2006+0.0945j  0.5269-0.099j  -0.2327+0.2757j  0.008 +0.1147j\n",
      "  -0.0506+0.092j  -0.1041-0.2304j -0.3972+0.4805j -0.0954-0.2177j]\n",
      " [-0.1295-0.1587j  0.1815+0.1986j -0.0996-0.1983j -0.3697+0.4263j\n",
      "  -0.1729-0.019j  -0.4346+0.3077j  0.2055+0.1765j  0.1376+0.3344j]\n",
      " [-0.3724-0.5003j -0.1125-0.2725j -0.0402+0.2994j -0.3849-0.0267j\n",
      "   0.393 +0.2562j  0.1423+0.1155j -0.0635-0.1336j -0.0753+0.0523j]\n",
      " [-0.1528+0.2489j -0.0655+0.3764j  0.1486+0.1059j -0.2019-0.1234j\n",
      "  -0.3107+0.3058j  0.3503+0.3919j  0.0303+0.1637j -0.4256-0.0662j]\n",
      " [-0.0238-0.1727j -0.3071+0.078j   0.1853-0.2987j  0.0207+0.0415j\n",
      "   0.0188+0.4263j  0.1027-0.1532j  0.0292+0.4863j  0.4608-0.2788j]\n",
      " [ 0.1624+0.2351j  0.2311-0.0187j  0.1415-0.227j  -0.1423-0.3131j\n",
      "   0.5125-0.1013j -0.0614+0.5134j -0.2805+0.1268j  0.1911-0.0567j]\n",
      " [-0.0476+0.3983j -0.4043-0.1464j -0.6899-0.1172j -0.0803-0.118j\n",
      "   0.0564+0.1781j -0.0128-0.0217j -0.0923+0.1006j  0.0269+0.2987j]\n",
      " [ 0.3913+0.092j  -0.2228-0.1365j  0.0504-0.0933j -0.1269+0.5538j\n",
      "   0.0821+0.2279j -0.1875+0.0743j -0.3142-0.1974j -0.3094-0.3214j]]\n",
      "\n",
      "Circuit:\n",
      "\n",
      "Layer 1\n",
      "\n",
      "theta: 1.014, phi: 0.25, target: (5, 6)\n",
      "theta: 0.859, phi: 0.93, target: (6, 7)\n",
      "\n",
      "Layer 2\n",
      "\n",
      "theta: 0.869, phi: 1.48, target: (3, 4)\n",
      "theta: 1.152, phi: 3.80, target: (4, 5)\n",
      "theta: 1.222, phi: 3.01, target: (5, 6)\n",
      "theta: 1.207, phi: 0.50, target: (6, 7)\n",
      "\n",
      "Layer 3\n",
      "\n",
      "theta: 1.211, phi: 4.92, target: (1, 2)\n",
      "theta: 0.706, phi: 6.17, target: (2, 3)\n",
      "theta: 1.168, phi: 6.00, target: (3, 4)\n",
      "theta: 1.482, phi: 1.23, target: (4, 5)\n",
      "theta: 0.983, phi: 4.18, target: (5, 6)\n",
      "theta: 0.860, phi: 3.11, target: (6, 7)\n",
      "\n",
      "Layer 4\n",
      "\n",
      "theta: 1.294, phi: 2.22, target: (0, 1)\n",
      "theta: 0.949, phi: 4.89, target: (1, 2)\n",
      "theta: 1.519, phi: 2.29, target: (2, 3)\n",
      "theta: 1.284, phi: 2.41, target: (3, 4)\n",
      "theta: 1.242, phi: 3.30, target: (4, 5)\n",
      "theta: 1.226, phi: 3.18, target: (5, 6)\n",
      "theta: 0.364, phi: 5.82, target: (6, 7)\n",
      "\n",
      "Layer 5\n",
      "\n",
      "theta: 1.369, phi: 2.81, target: (0, 1)\n",
      "theta: 0.861, phi: 4.16, target: (1, 2)\n",
      "theta: 1.240, phi: 1.06, target: (2, 3)\n",
      "theta: 0.855, phi: 4.66, target: (3, 4)\n",
      "theta: 0.532, phi: 1.24, target: (4, 5)\n",
      "\n",
      "Layer 6\n",
      "\n",
      "theta: 0.946, phi: 2.99, target: (0, 1)\n",
      "theta: 0.842, phi: 1.56, target: (1, 2)\n",
      "theta: 0.830, phi: 3.45, target: (2, 3)\n",
      "\n",
      "Layer 7\n",
      "\n",
      "theta: 0.994, phi: 1.25, target: (0, 1)\n",
      "\n",
      "Reconstructed Unitary:\n",
      " [[-0.2006+0.0945j  0.5269-0.099j  -0.2327+0.2757j  0.008 +0.1147j\n",
      "  -0.0506+0.092j  -0.1041-0.2304j -0.3972+0.4805j -0.0954-0.2177j]\n",
      " [-0.1295-0.1587j  0.1815+0.1986j -0.0996-0.1983j -0.3697+0.4263j\n",
      "  -0.1729-0.019j  -0.4346+0.3077j  0.2055+0.1765j  0.1376+0.3344j]\n",
      " [-0.3724-0.5003j -0.1125-0.2725j -0.0402+0.2994j -0.3849-0.0267j\n",
      "   0.393 +0.2562j  0.1423+0.1155j -0.0635-0.1336j -0.0753+0.0523j]\n",
      " [-0.1528+0.2489j -0.0655+0.3764j  0.1486+0.1059j -0.2019-0.1234j\n",
      "  -0.3107+0.3058j  0.3503+0.3919j  0.0303+0.1637j -0.4256-0.0662j]\n",
      " [-0.0238-0.1727j -0.3071+0.078j   0.1853-0.2987j  0.0207+0.0415j\n",
      "   0.0188+0.4263j  0.1027-0.1532j  0.0292+0.4863j  0.4608-0.2788j]\n",
      " [ 0.1624+0.2351j  0.2311-0.0187j  0.1415-0.227j  -0.1423-0.3131j\n",
      "   0.5125-0.1013j -0.0614+0.5134j -0.2805+0.1268j  0.1911-0.0567j]\n",
      " [-0.0476+0.3983j -0.4043-0.1464j -0.6899-0.1172j -0.0803-0.118j\n",
      "   0.0564+0.1781j -0.0128-0.0217j -0.0923+0.1006j  0.0269+0.2987j]\n",
      " [ 0.3913+0.092j  -0.2228-0.1365j  0.0504-0.0933j -0.1269+0.5538j\n",
      "   0.0821+0.2279j -0.1875+0.0743j -0.3142-0.1974j -0.3094-0.3214j]]\n",
      "\n",
      "Fidellity:\n",
      " 0.99999992328784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/86/5xcctjn15052xcf5hwdbv_6h0000gp/T/ipykernel_45199/511947677.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  theta_t = torch.tensor(theta, dtype=torch.float32, device=device, requires_grad=False)\n",
      "/var/folders/86/5xcctjn15052xcf5hwdbv_6h0000gp/T/ipykernel_45199/511947677.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  phi_t = torch.tensor(phi, dtype=torch.float32, device=device, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "def fidellity(U_exp: Tensor, U: Tensor):\n",
    "    return torch.abs(torch.einsum('ii', U_exp.adjoint() @ U)).item()/U.shape[0]\n",
    "\n",
    "def haar_measure(N):\n",
    "    \"\"\"Generate a Haar-random matrix using the QR decomposition.\"\"\"\n",
    "    A, B = torch.randn(size=(N, N), dtype=torch.float64), torch.randn(size=(N, N), dtype=torch.float64)\n",
    "    Z = A + 1j * B\n",
    "    Q, R = torch.linalg.qr(Z)\n",
    "    Lambda = torch.diag(torch.tensor([R[i, i] / torch.abs(R[i, i]) for i in range(N)]))\n",
    "\n",
    "    return Q @ Lambda\n",
    "\n",
    "N = torch.randint(low=2, high=20, size = ())\n",
    "U = haar_measure(N)\n",
    "\n",
    "print(\"Initial Unitary:\\n\", U.numpy().round(4))\n",
    "\n",
    "# Apply Clements decomposition\n",
    "decomposition = clements_decomposition(U)\n",
    "\n",
    "# Print the decomposition parameters\n",
    "print(\"\\nCircuit:\\n\")\n",
    "num_layer = 1\n",
    "temp = 0\n",
    "print(f'Layer {num_layer}\\n')\n",
    "for theta, phi, target in decomposition.circuit:\n",
    "    if temp > target[0]:\n",
    "        num_layer += 1\n",
    "        print(f'\\nLayer {num_layer}\\n')\n",
    "    print(f\"theta: {theta:.3f}, phi: {phi:.2f}, target: {target}\")\n",
    "    temp = target[0]\n",
    "\n",
    "# Reconstruct the unitary from the decomposition\n",
    "reconstructed_unitary = circuit_reconstruction(decomposition)\n",
    "\n",
    "# Print the reconstructed unitary and assert initial matrix\n",
    "print(\"\\nReconstructed Unitary:\\n\", reconstructed_unitary.numpy().round(4))\n",
    "print(\"\\nFidellity:\\n\", fidellity(reconstructed_unitary, U))\n",
    "assert torch.allclose(U, reconstructed_unitary, atol=1e-06), \"Reconstructed unitary does not match original.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка (Рек)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Unitary:\n",
      " [[ 0.0073+0.2203j -0.1411-0.0886j  0.061 +0.4463j  0.3052+0.0673j\n",
      "   0.2207-0.1099j  0.2861+0.3648j  0.5891-0.0168j]\n",
      " [-0.1708-0.1095j  0.0113+0.1097j -0.5437+0.1249j  0.4255-0.0532j\n",
      "   0.4129-0.1876j  0.1136-0.4419j -0.1646-0.1037j]\n",
      " [ 0.5238+0.3781j  0.1524+0.0299j -0.2022-0.1863j  0.0586-0.1991j\n",
      "   0.1443+0.2397j -0.4261-0.1808j  0.3528-0.1514j]\n",
      " [-0.0537+0.4461j -0.3094+0.0577j -0.138 -0.122j   0.3473+0.1255j\n",
      "  -0.2015-0.09j   -0.1966+0.4026j -0.4233-0.3164j]\n",
      " [-0.3494-0.2535j -0.0515+0.5575j  0.1013+0.1824j  0.1135-0.1974j\n",
      "   0.0013+0.5223j -0.2371+0.1213j  0.1165-0.218j ]\n",
      " [-0.0466+0.1526j -0.1023+0.5499j -0.3911-0.0521j -0.5374+0.0769j\n",
      "  -0.1116-0.3511j  0.1269+0.0644j  0.2241-0.0712j]\n",
      " [ 0.1117+0.2583j -0.456 -0.0519j  0.0994+0.409j  -0.3873+0.2057j\n",
      "   0.3112+0.3183j  0.0217-0.2652j -0.2629-0.0508j]]\n",
      "\n",
      "Circuit:\n",
      "\n",
      "\n",
      "Layer 1\n",
      "\n",
      "theta: 0.647, phi: 4.80, target: (0, 1)\n",
      "\n",
      "Layer 2\n",
      "\n",
      "theta: 1.021, phi: -4.38, target: (0, 2)\n",
      "\n",
      "Layer 3\n",
      "\n",
      "theta: 0.993, phi: -5.39, target: (1, 2)\n",
      "\n",
      "Layer 4\n",
      "\n",
      "theta: 0.534, phi: -2.65, target: (0, 3)\n",
      "\n",
      "Layer 5\n",
      "\n",
      "theta: 1.037, phi: -1.83, target: (1, 3)\n",
      "\n",
      "Layer 6\n",
      "\n",
      "theta: 0.366, phi: -0.74, target: (2, 3)\n",
      "\n",
      "Layer 7\n",
      "\n",
      "theta: 0.382, phi: 1.08, target: (0, 4)\n",
      "\n",
      "Layer 8\n",
      "\n",
      "theta: 0.546, phi: 1.41, target: (1, 4)\n",
      "\n",
      "Layer 9\n",
      "\n",
      "theta: 0.345, phi: -0.58, target: (2, 4)\n",
      "\n",
      "Layer 10\n",
      "\n",
      "theta: 1.018, phi: 3.12, target: (3, 4)\n",
      "\n",
      "Layer 11\n",
      "\n",
      "theta: 0.611, phi: -2.26, target: (0, 5)\n",
      "\n",
      "Layer 12\n",
      "\n",
      "theta: 0.737, phi: 0.01, target: (1, 5)\n",
      "\n",
      "Layer 13\n",
      "\n",
      "theta: 0.488, phi: 1.57, target: (2, 5)\n",
      "\n",
      "Layer 14\n",
      "\n",
      "theta: 0.790, phi: -3.54, target: (3, 5)\n",
      "\n",
      "Layer 15\n",
      "\n",
      "theta: 1.262, phi: -4.29, target: (4, 5)\n",
      "\n",
      "Layer 16\n",
      "\n",
      "theta: 0.630, phi: -2.92, target: (0, 6)\n",
      "\n",
      "Layer 17\n",
      "\n",
      "theta: 0.243, phi: -0.37, target: (1, 6)\n",
      "\n",
      "Layer 18\n",
      "\n",
      "theta: 0.512, phi: -2.55, target: (2, 6)\n",
      "\n",
      "Layer 19\n",
      "\n",
      "theta: 0.884, phi: -0.45, target: (3, 6)\n",
      "\n",
      "Layer 20\n",
      "\n",
      "theta: 0.606, phi: -1.87, target: (4, 6)\n",
      "\n",
      "Layer 21\n",
      "\n",
      "theta: 0.721, phi: -2.64, target: (5, 6)\n",
      "\n",
      "Reconstructed Unitary:\n",
      " tensor([[ 0.0073+0.2203j, -0.1411-0.0886j,  0.0610+0.4463j,  0.3052+0.0673j,\n",
      "          0.2207-0.1099j,  0.2861+0.3648j,  0.5891-0.0168j],\n",
      "        [-0.1708-0.1095j,  0.0113+0.1097j, -0.5437+0.1249j,  0.4255-0.0532j,\n",
      "          0.4129-0.1876j,  0.1136-0.4419j, -0.1646-0.1037j],\n",
      "        [ 0.5238+0.3781j,  0.1524+0.0299j, -0.2022-0.1863j,  0.0586-0.1991j,\n",
      "          0.1443+0.2397j, -0.4261-0.1808j,  0.3528-0.1514j],\n",
      "        [-0.0537+0.4461j, -0.3094+0.0577j, -0.1380-0.1220j,  0.3473+0.1255j,\n",
      "         -0.2015-0.0900j, -0.1966+0.4026j, -0.4233-0.3164j],\n",
      "        [-0.3494-0.2535j, -0.0515+0.5575j,  0.1013+0.1824j,  0.1135-0.1974j,\n",
      "          0.0013+0.5223j, -0.2371+0.1213j,  0.1165-0.2180j],\n",
      "        [-0.0466+0.1526j, -0.1023+0.5499j, -0.3911-0.0521j, -0.5374+0.0769j,\n",
      "         -0.1116-0.3511j,  0.1269+0.0644j,  0.2241-0.0712j],\n",
      "        [ 0.1117+0.2583j, -0.4560-0.0519j,  0.0994+0.4090j, -0.3873+0.2057j,\n",
      "          0.3112+0.3183j,  0.0217-0.2652j, -0.2629-0.0508j]],\n",
      "       dtype=torch.complex128)\n",
      "\n",
      "Fidellity:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "N = torch.randint(low=2, high=20, size = ())\n",
    "U = haar_measure(N)\n",
    "\n",
    "print(\"Initial Unitary:\\n\", U.numpy().round(4))\n",
    "thetas, phis, targets, phases = reck_decomposition(U)\n",
    "\n",
    "# Print the decomposition parameters\n",
    "print(\"\\nCircuit:\\n\")\n",
    "\n",
    "for i in range(len(thetas) - 1, -1, -1):\n",
    "    print(f'\\nLayer {len(thetas) - i}\\n')\n",
    "    theta = thetas[i]\n",
    "    phi = phis[i]\n",
    "    m, n = targets[i].tolist()\n",
    "    print(f\"theta: {theta:.3f}, phi: {phi:.2f}, target: {(m, n)}\")\n",
    "\n",
    "reconstructed_unitary = get_matrix(N, phases, phis, thetas, targets)\n",
    "\n",
    "# Print the reconstructed unitary and assert initial matrix\n",
    "print(\"\\nReconstructed Unitary:\\n\", reconstructed_unitary)\n",
    "print(\"\\nFidellity:\\n\", fidellity(reconstructed_unitary, U))\n",
    "assert torch.allclose(U, reconstructed_unitary, atol=1e-06), \"Reconstructed unitary does not match original.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
